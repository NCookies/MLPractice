{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-linear 문제 해결\n",
    "- non-linear 문제를 해결하기 위해서는 Logistic Regression 이 적합하지 않음\n",
    "- polynomials 로 해결할 수는 있지만 feature 를 뽑는 번거로움을 겪어야 함\n",
    "- 이럴 때 feature engineering 을 신경써야 함\n",
    "- neural network의 hidden layer 가 이 번거로움을 해결해 줄 것임\n",
    "\n",
    "<img src=\"https://github.com/dennybritz/nn-from-scratch/blob/master/nn-3-layer-network.png?raw=true\" />\n",
    "### neural network 의 구조\n",
    "- 이 예제에서는 input layer 노드 2개, hidden layer 노드 5개, output layer 노드 2개로 이루어진 neural network 를 사용할 것임\n",
    "- output 은 0(female), 1(male) 로 두 가지임\n",
    "- hidden layer 의 차원(노드의 개수)를 증가시킴으로써 정확도를 높일 수 있지만, 더 많은 계산양을 요구하고 overfitting 되기 쉬워짐\n",
    "- 이 hidden layer 의 개수를 정하는 것은 해결하려는 문제에 따라 다름\n",
    "- 명확하게 정해져있지는 않은데, 사실 이것은 science 보다는 art 에 가까움\n",
    "\n",
    "### activation function\n",
    "- hidden layer 를 위한 activation function 을 정해야함\n",
    "- tanh, sigmoid function, RELU 등이 있음\n",
    "- 여기서는 tanh를 사용할 것임\n",
    "- tanh는 꽤 많은 상황에 적용할 수 있음\n",
    "- 이 tanh 에서 nice 한 점은 미분한 값을 원래 함수 값을 통해 다시 계산할 수 있다는 것임\n",
    "- 예를 들어 tanh x 의 미분값은 1 - tanh x ^ 2 인데, 한 번만 계산하여 값을 다시 사용할 수 있음\n",
    "\n",
    "### softmax\n",
    "- 이런 미분을 하는 이유는, 나중에 output layer 에서 확률값을 얻기 위해 softmax를 사용할 것이기 때문임\n",
    "- softmax 는 일반 값을 확률값으로 바꿔줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW OUR NETWORK MAKES PREDICTIONS\n",
    "- forward propagation 을 이용하여 값을 예측할 것임\n",
    "- 이것은 단순히 행렬 곱의 묶음이고, 우리가 위에서 정의한 activation function 을 적용한 것임\n",
    "- 식은 다음과 같음(자세한 설명은 생략)\n",
    "<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++z_1+%26+%3D+xW_1+%2B+b_1+%5C%5C++a_1+%26+%3D+%5Ctanh%28z_1%29+%5C%5C++z_2+%26+%3D+a_1W_2+%2B+b_2+%5C%5C++a_2+%26+%3D+%5Chat%7By%7D+%3D+%5Cmathrm%7Bsoftmax%7D%28z_2%29++%5Cend%7Baligned%7D&bg=ffffff&fg=000&s=0\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING THE PARAMETERS\n",
    "- parameter 를 학습한다는 것은 training data에서 error 를 최소화하는 값(W1, b1, W2, b2)을 찾는다는 것임\n",
    "- 이 error 를 줄이는 function 을 보통 error 또는 cost function 이라고 함\n",
    "- softmax 와 함께 사용되는 것은 주로 cross-entropy loss(negative log likelihood) 임\n",
    "- N개의 training examples 와 C개의 class 가 있을 때 식은 다음과 같음\n",
    "<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++L%28y%2C%5Chat%7By%7D%29+%3D+-+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn+%5Cin+N%7D+%5Csum_%7Bi+%5Cin+C%7D+y_%7Bn%2Ci%7D+%5Clog%5Chat%7By%7D_%7Bn%2Ci%7D++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\" />\n",
    "- 공식은 굉장히 복잡해 보이지만, 실제로 하는 것은 training examples를 합치고 만약 잘못된 값으로 에측했다면 그것을 loss에 더하는 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}